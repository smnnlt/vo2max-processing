---
title: ''
output: github_document
bibliography: ../references.bib
---

# Analysis Script for the Systematic Review

## Project: 'Data Processing Strategies to Determine Maximum Oxygen Uptake: A Systematic Scoping Review and Experimental Comparison'

This script closely follows the preregistration, which is uploaded on the [OSF](https://osf.io/3am4s).

```{r setup, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(out.width = "70%", fig.align = "center")

# Packages used for the data workflow
library(readxl)
library(rentrez)
library(XML)
library(purrr)
library(dplyr)
library(ggplot2)
library(MetBrewer)
library(treemapify)
library(colorspace)
```

#### Get search data

The search was conducted on the 16th March 2022 at around 10 pm CET.

The Pubmed results are saved in the file 'pubmed.csv'; the Web of Science results are saved in the file 'wos.xls'.

```{r read, warning=FALSE, message=FALSE}
# file name for PubMed results
pm_file <- "../data/search/pubmed.csv"

# file name for merged WoS results
wos_file <- "../data/search/wos.xls"

# read PubMed data
pm_data_pre <- read.csv(pm_file)
pm_data <- data.frame(
  pmid = pm_data_pre[, 1],
  title = pm_data_pre$Title,
  authors = pm_data_pre$Authors,
  journal = pm_data_pre$Journal.Book,
  year = pm_data_pre$Publication.Year,
  doi = pm_data_pre$DOI,
  abstract = NA
)
# read Web of Science data
wos_data_pre <- readxl::read_xls(wos_file)
wos_data <- data.frame(
  pmid = wos_data_pre$`Pubmed Id`,
  title = wos_data_pre$`Article Title`,
  authors = wos_data_pre$`Author Full Names`,
  journal = wos_data_pre$`Source Title`,
  year = wos_data_pre$`Publication Year`,
  doi = wos_data_pre$DOI,
  abstract = wos_data_pre$Abstract
)

# count PubMed results
pm_count <- nrow(pm_data)
# count Web of Science results
wos_count <- nrow(wos_data)
overall_count <- pm_count + wos_count
```

The search yielded `r overall_count` results (Pubmed: `r pm_count`; WOS: `r wos_count`)

#### Removal of duplicates

```{r duplicates}
# Merge data frame
merge_data <- rbind(wos_data, pm_data)

# Filter entries without doi
no_doi <- is.na(merge_data$doi) | (merge_data$doi == "")
merge_data <- merge_data[!no_doi, ]

# Count removal from missing doi
no_doi_count <- sum(no_doi)

# Find duplicated DOIs
dupl <- duplicated(merge_data$doi)
# Filter duplicates
merge_data <- merge_data[!dupl, ]

# Count removal of duplicates
dupl_count <- sum(dupl)
```

Removed because of missing DOI: `r no_doi_count`

Duplicates removed: `r dupl_count`

Remaining: `r overall_count - no_doi_count - dupl_count`

#### Automated title filtering

```{r filter}
# Filter titles by terms
terms <- paste0("review|comment|correction|retraction|meta-analysis",
                "|editorial|erratum|reply")
excl <- grepl(terms, merge_data$title, ignore.case = TRUE)
filtered_data <- merge_data[!excl, ]

# Count removal by title filtering
title_excl_count <- sum(excl)
# count remaining articles
forsample_count <- nrow(filtered_data)
```

Removed by automated title filtering: `r title_excl_count`

Remaining articles for random sampling: `r forsample_count`

#### Random Sampling

```{r sampling, eval=FALSE}
# Assign ids to articles left
filtered_data$id <- seq_len(nrow(filtered_data))

# randomly sample 500 articles
set.seed(4711)
smpl <- sample(filtered_data$id, size = 500)

sampled_data <- filtered_data[smpl, ]
# assign sample id
sampled_data$sid <- seq_len(nrow(sampled_data))

# get abstract from PubMed where missing

no_abstract <- is.na(sampled_data$abstract)

no_abstract_pmid <- sampled_data$pmid[no_abstract]

# function to scrape abstract from the PubMed API
get_abstract <- function(pmid) {
  if (is.na(pmid))
    return(NA)
  xml_data <- rentrez::entrez_fetch(
    db = "pubmed", 
    id = pmid, 
    rettype = "xml", 
    parse = TRUE
  )
  text <- XML::xmlValue(XML::getNodeSet(xml_data, "//AbstractText"))
  out <- paste(text, collapse = "///")
  if (out == "") out <- NA
  out
}

# retrieve missing abstracts
sampled_data$abstract[no_abstract] <- purrr::map_chr(
  .x = no_abstract_pmid, 
  .f = get_abstract
)

# manually add abstract for articles, where automated abstract retrieval did not
# work. In these cases no blinding applies to the principal investigator
which(is.na(sampled_data$abstract))

sampled_data$abstract[50] <- readtext::readtext("../data/search/a50.txt")$text
sampled_data$abstract[238] <- readtext::readtext("../data/search/a238.txt")$text
sampled_data$abstract[288] <- readtext::readtext("../data/search/a288.txt")$text
sampled_data$abstract[416] <- readtext::readtext("../data/search/a416.txt")$text
sampled_data$abstract[488] <- readtext::readtext("../data/search/a488.txt")$text
sampled_data$abstract[490] <- readtext::readtext("../data/search/a490.txt")$text
sampled_data$abstract[500] <- readtext::readtext("../data/search/a500.txt")$text

# later plotting throws an error for some articles due to unrecognized html-tags
# in the abstracts. These abstract are replaced by their text equivalents
sampled_data$abstract[344] <- readtext::readtext("../data/search/a344.txt")$text
sampled_data$abstract[356] <- readtext::readtext("../data/search/a356.txt")$text

# save(sampled_data, file = "../data/screen/sample.Rda")
```

#### Prepare Title-Abstract plots for screening

```{r abstractplots, eval=FALSE}
load("../data/screen/sample.Rda")

# Function for creating plots with blinded abstract data
create_abstract_plot <- function(row, data) {
  # get random colour
  # different colours in plots are used to stay concentrated when scanning
  # the abstracts
  clrs <- c(MetBrewer::met.brewer("Moreau")[-4])
  clr <- sample(clrs, 1)
  
  g <- ggplot(data = NULL, aes(x = 0, y = 1)) +
  ggtext::geom_textbox(
    width = 1, 
    box.colour = "transparent", 
    label =  gsub(">", "", data$abstract[row]),
    colour = clr
  ) +
  labs(
    title = paste0(strwrap(data$title[row], 80), collapse = "\n"), 
    subtitle = paste0("Sampling ID: ", sprintf("%03i", data$sid[row])),
    caption = paste0("ID: ", data$id[row], "; PMID = ", data$pmid[row])
    ) +
  theme_void()
  
  ggsave(
    filename = paste0("../plots/abstracts/", sprintf("%03i", data$sid[row]), ".png"), 
    plot = g, 
    dpi = 300, width = 7, height = 5, bg = "white"
  )
}


# Create blinded abstract plots
# purrr::walk(
#   .x = seq_len(nrow(sampled_data)),
#   .f = create_abstract_plot, 
#   data = sampled_data
# )

```

#### Compare abstract screening results

```{r abstract-screen}
load("../data/screen/sample.Rda")
# read screening results
s_res1 <- read.csv("../data/screen/abstract_screening_i1.csv")[, 2]
s_res2 <- read.csv("../data/screen/abstract_screening_i2.csv")[, 2]

# compare results
comp_res <- function(x1, x2) {
  if (is.na(x1)) x1c <- 0 else x1c <- x1
  if (is.na(x2)) x2c <- 0 else x2c <- x2
  if (x1c == x2c) out <- x1 else out <- "XXX"
  out
}

s_res <- purrr::map2_chr(s_res1, s_res2, comp_res)
s_res_df <- data.frame(
  id = 1:500,
  i1 = s_res1,
  i2 = s_res2,
  result = s_res,
  decision = rep.int("", 500),
  doi = sampled_data$doi
)
# write.csv(s_res_df, "../data/screen/abstract_screening.csv")

# Calculate relative agreement
1 - (sum(s_res == "XXX", na.rm = TRUE) / length(s_res))

# Get final decisions
s_res_final <- read.csv("../data/screen/abstract_screening.csv")
# number excluded 
abstr_excl_count <- sum(!is.na(s_res_final$decision))
# reasons for exclusion
table(s_res_final$decision)
# extract remaining data
ft_screen <- sampled_data[is.na(s_res_final$decision), ]

# write file for full text screening
ft_screen_df <- data.frame(
  sid = ft_screen$sid,
  fulltext_exclusion = rep.int("", nrow(ft_screen)),
  exclusion_note = rep.int("", nrow(ft_screen)),
  doi = ft_screen$doi
)

# write.csv(ft_screen_df, file = "../data/screen/fulltext_screening_i1.csv", row.names = FALSE)
```

Removed by abstract screening: `r abstr_excl_count`

#### Compare full text screening results

```{r fulltext-screening}
# read screening results
f_res_i2 <- read.csv("../data/screen/fulltext_screening_i2.csv")
f_res_i1 <- read.csv("../data/screen/fulltext_screening_i1.csv", sep = ";")

f_res1 <- f_res_i1$fulltext_exclusion
f_res2 <- f_res_i2$fulltext_exclusion

f_res <- purrr::map2_chr(f_res1, f_res2, comp_res)
f_res_df <- data.frame(
  sid = f_res_i2$sid,
  i1 = f_res1,
  i2 = f_res2,
  result = f_res,
  decision = rep.int("", length(f_res1)),
  i1_note = f_res_i1$exclusion_note,
  i2_note = f_res_i2$exclusion_note,
  doi = f_res_i1$doi
)

# write.csv(f_res_df, "../data/screen/fulltext_screening.csv", row.names = FALSE)

# Calculate relative agreement
1 - (sum(f_res == "XXX", na.rm = TRUE) / length(f_res))

1 - ((sum(is.na(f_res1) | is.na(f_res2)) - sum(is.na(f_res))) / length(f_res))

# read final decision
ft_res <- read.csv("../data/screen/fulltext_screening.csv")
ft_res$decision <- factor(
  ft_res$decision, 
  levels = c("t", "l", "r", "h", "m", "c", "e", "d", "o"),
  labels = c("no fulltext", "no english full-text", "no original research", 
             "no humans", "VO2 not measured", "VO2 not continously measured", 
             "no exhaustion", "long protocol", "other")
)

# count exclusions
ft_excl_count <- sum(!is.na(ft_res$decision))

# count remaining articles for data extraction
final_count <- sum(is.na(ft_res$decision))

table(ft_res$decision, useNA = "ifany")

```

Excluded after full text screening: `r ft_excl_count`

Remaining articles: `r final_count`

#### Prepare data extraction

```{r extraction, eval=FALSE}
emp <- rep.int("", final_count)

extr <- data.frame(
  sid = ft_res$sid[is.na(ft_res$decision)],
  cart = emp,
  type = emp,
  outcome = emp,
  pre = emp,
  software = emp,
  interpolation = emp,
  ptype = emp,
  averaging = emp,
  parameters = emp,
  source = emp,
  doi = ft_res$doi[is.na(ft_res$decision)]
)

# write.csv(extr, "../data/extract/extraction.csv", row.names = FALSE)
```


#### Read and clean extracted data

```{r clean, warning=FALSE}
ext <- read.csv("../data/extract/extraction.csv")

# Create column for processing type + averaging type combination
ext$proc_combination <- paste(ext$ptype, ext$averaging, sep = "-")
ext$proc_combination[ext$proc_combination == "NA-NA"] <- NA

# Separate parameter column for multiple parameters (i.e. multiple binned averages)
# and calculate total duration

split_parameters <- function(n) {
  value <- ext$parameters[n]
  if (grepl("x", value)) {
    value_split <- strsplit(value, "x")[[1]]
    out <- data.frame(
      param_times = as.numeric(value_split[1]), 
      param_value = as.numeric(value_split[2])
    )
    out$param_duration <- out$param_times * out$param_value
  } else {
    out <- data.frame(
      param_times = NA, 
      param_value = as.numeric(value), 
      param_duration = as.numeric(value)
    )
  }
  out
}

ext <- cbind(ext, purrr::map_dfr(seq_len(nrow(ext)), split_parameters))

# save data
# save(ext, file = "../data/review.Rda")

# Show metabolic carts used
table(ext$type, useNA = "ifany")
```

#### Analyze extracted data


```{r percentages}
load("../data/review.Rda")

n_total <- nrow(ext)
ext_bbb <- ext[ext$type == "bbb" & !is.na(ext$type),]
n_bbb <- nrow(ext_bbb)

# Type of metabolic cart
p_cart <- 1 - (sum(grepl("NA", ext$cart) | is.na(ext$cart)) / n_total)

# Preprocessing
p_pre <- sum(!is.na(ext_bbb$pre)) / n_bbb

# Software
p_software <- sum(!is.na(ext_bbb$software)) / n_bbb

# Processing strategy
p_proc <- sum(!is.na(ext$ptype) & !is.na(ext$averaging) & !is.na(ext$parameters)) / n_total

# Source
p_source <- sum(!is.na(ext$source)) / n_total

reporting <- data.frame(
  "cart" = p_cart,
  "pre" = p_pre,
  "software" = p_software,
  "processing" = p_proc,
  "source" = p_source
)

# save(reporting, file = "../data/reporting.Rda")

knitr::kable(purrr::map_dfr(reporting, scales::percent, accuracy = 0.1))
```

#### Plot: Calculation intervals

```{r interval, warning = FALSE}
# Show data processing strategies overall
table(ext$ptype, useNA = "ifany")

# Show data processing type for breath-by-breath measurements only
table(ext$proc_combination[ext$type == "bbb"], useNA = "ifany")

# Plot for total calculation interval lengths
p_int <- ext[ext$ptype == "time" & !is.na(ext$ptype), ] |>
  ggplot(aes(y = param_duration)) +
    geom_bar() +
  geom_text(aes(label = ..count..), stat = "count", hjust = 1.2, colour = "white") +
    scale_y_reverse(
      breaks = c(5, 10, 15, 20, 30, 45, 60),
      minor_breaks = NULL,
      name = "Duration (s)"
    ) +
    scale_x_continuous(
      minor_breaks = NULL,
      name = "Count",
      breaks = c(0, 10, 20, 30, 40),
      labels = NULL,
      expand = c(0, 0),
      position = "top"
    ) +
    theme_minimal(13) +
   theme(
     panel.grid.major.y = element_blank(),
     axis.ticks.y = element_line(colour = "grey", size = 0.5)
    )

# ggsave("../plots/duration_count.png", plot = p_int, width = 5, height = 3, dpi = 600, bg = "white")

knitr::include_graphics("../plots/duration_count.png")

# save data for table generation as alternative to a plot
i_data <- ext[ext$ptype == "time" & !is.na(ext$ptype), ]
save(i_data, file = "../data/intervals.Rda")
```

#### Plot: Processing strategies

```{r strategies}

ext_count <- ext |> 
  filter(type == "bbb") |>
  select(c(proc_combination, ptype)) |>
  filter(!is.na(proc_combination)) |>
  count(proc_combination, ptype)

ext_count$proc_combination <- factor(
  ext_count$proc_combination, 
  levels = c("breath-binned", "time-moving", "breath-moving", "time-binned", "time-mbinned"),
  labels = c("Binned Breath","Moving Time", "Moving Breath", "Binned Time", "Multiple Binned Time"))

percs <- scales::percent(ext_count$n / sum(ext_count$n))
ext_count$perc <- percs
ext_count$label <- paste0(ext_count$proc_combination, " (", percs, ")")
ext_count$subgroup <- c(1, 1, 2, 3, 4)

# save(ext_count, file = "../data/bbb_strategies.Rda")

p_strat <- ggplot(ext_count, aes(area = n, fill = proc_combination, label = label, subgroup = subgroup)) +
  geom_treemap(show.legend = FALSE, start = "topleft") +
  geom_treemap_text(
    color = "black",
    reflow = TRUE, 
    place = "centre", 
    start = "topleft",
    padding.y = grid::unit(1.5, "mm")) +
  scale_fill_manual(
    values = colorspace::lighten(c("#663171", "#ea7428", "#653151", "#0c7156", "#0c7182"), amount = 0.2)
  )

# ggsave("../plots/strategies.png", plot = p_strat, width = 5, height = 3, dpi = 600, bg = "white")

knitr::include_graphics("../plots/strategies.png")
```

#### Create data of all sampled studies

```{r}
load("../data/screen/sample.Rda")
abstract <- read.csv("../data/screen/abstract_screening.csv")
fulltext <- read.csv("../data/screen/fulltext_screening.csv")
fullsample <- sampled_data[, c("sid", "title", "authors", "year", "doi")]
get_status <- function(sid) {
  if (!(sid %in% fulltext$sid)) {
    status <- "excluded_abstract"
    reason <- abstract$decision[sid]
  } else {
    if (is.na(fulltext$decision[sid])) {
      status <- "included"
      reason <- NA
    } else {
      status <- "excluded_fulltext"
      reason <- fulltext$decision[sid]
    }
  }
  data.frame(status = status, reason = reason)
}
fullsample <- cbind(
  fullsample, 
  purrr::map_dfr(seq_len(nrow(fullsample)), get_status)
)

fullsample$reason <- factor(
  fullsample$reason, 
  levels = c("t", "l", "r", "h", "m", "c", "e", "d", "o"),
  labels = c("report not retrieved", "not in English", "no primary research", 
             "reasearch not in humans", "VO2 only estimated", "no appropriate test protocol (VO2 not continously measured)", 
             "no appropriate test protocol (no exhaustion)", "no appropriate test protocol (longer than 20 minutes)", "no information regarding the exclusion criteria")
)

write.csv(fullsample, file = "../data/fullsample.csv", na = "", row.names = FALSE)
```

